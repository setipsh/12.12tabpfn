{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scripts.differentiable_pfn_evaluation import eval_model_range\n",
    "from scripts.model_builder import get_model, get_default_spec, save_model, load_model\n",
    "from scripts.transformer_prediction_interface import transformer_predict, get_params_from_config, load_model_workflow\n",
    "\n",
    "from scripts.model_configs import *\n",
    "\n",
    "from datasets import load_openml_list, open_cc_dids, open_cc_valid_dids\n",
    "from priors.utils import plot_prior, plot_features\n",
    "from priors.utils import uniform_int_sampler_f\n",
    "\n",
    "from scripts.tabular_metrics import calculate_score_per_method, calculate_score\n",
    "from scripts.tabular_evaluation import evaluate\n",
    "\n",
    "from priors.differentiable_prior import DifferentiableHyperparameterList, draw_random_style, merge_style_with_info\n",
    "from scripts import tabular_metrics\n",
    "from notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_datasets = True\n",
    "max_samples = 10000 if large_datasets else 5000\n",
    "bptt = 10000 if large_datasets else 3000\n",
    "suite='cc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "base_path = '.'\n",
    "max_features = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_models(model_string):\n",
    "    print(model_string)\n",
    "\n",
    "    for i in range(80):\n",
    "        for e in range(50):\n",
    "            exists = Path(os.path.join(base_path, f'models_diff/prior_diff_real_checkpoint{model_string}_n_{i}_epoch_{e}.cpkt')).is_file()\n",
    "            if exists:\n",
    "                print(os.path.join(base_path, f'models_diff/prior_diff_real_checkpoint{model_string}_n_{i}_epoch_{e}.cpkt'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(config_sample, i, add_name=''):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = time.time()\n",
    "    max_saved_models = 50  # 保持最多保存 50 个模型\n",
    "    \n",
    "    # 定义保存模型的回调函数\n",
    "    def save_callback(model, epoch):\n",
    "        # 仅保存第 91 到第 130 个 epoch 的模型\n",
    "        if epoch < 1 or epoch > 130:\n",
    "            return  # 跳过第 101 到第 150 以外的 epoch，不保存模型\n",
    "        \n",
    "        if not hasattr(model, 'last_saved_epoch'):\n",
    "            model.last_saved_epoch = 0\n",
    "\n",
    "        # 保存当前模型\n",
    "        config_sample['epoch_in_training'] = epoch\n",
    "        save_path = f'/root/autodl-fs/顺序注意力模型/prior_diff_real_checkpoint{add_name}_n_{i}_epoch_{epoch}.cpkt'\n",
    "        save_model(model, base_path, save_path, config_sample)\n",
    "        model.last_saved_epoch += 1  # 更新保存次数\n",
    "        \n",
    "        # 获取当前已保存的模型列表\n",
    "        saved_models = sorted(\n",
    "            [file for file in os.listdir(\"/root/autodl-fs/顺序注意力模型\") if file.startswith(f\"prior_diff_real_checkpoint{add_name}_n_{i}_\")],\n",
    "            key=lambda x: int(x.split('_epoch_')[-1].split('.cpkt')[0])  # 按 epoch 排序\n",
    "        )\n",
    "        \n",
    "        # 保持最多保存 50 个模型，删除最早的模型\n",
    "        if len(saved_models) > max_saved_models:\n",
    "            oldest_model = saved_models[0]\n",
    "            os.remove(os.path.join(\"/root/autodl-fs/顺序注意力模型\", oldest_model))  # 删除最早的模型文件\n",
    "        \n",
    "    # 初始化并训练模型\n",
    "    model = get_model(config_sample, device, should_train=True, verbose=1, epoch_callback=save_callback)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define prior settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reload_config(config_type='causal', task_type='multiclass', longer=0, use_sequential_attention=False):\n",
    "    config = get_prior_config(config_type=config_type)\n",
    "    \n",
    "    config['prior_type'], config['differentiable'], config['flexible'] = 'prior_bag', True, True\n",
    "    \n",
    "    model_string = ''\n",
    "    \n",
    "    config['epochs'] = 12000\n",
    "    config['recompute_attn'] = True\n",
    "\n",
    "    config['max_num_classes'] = 10\n",
    "    config['num_classes'] = uniform_int_sampler_f(2, config['max_num_classes'])\n",
    "    \n",
    "    # 打印 num_classes 确认其值  \n",
    "    print(f\"Number of classes: {config['num_classes']}\")  # 添加这一行 \n",
    "    \n",
    "    config['balanced'] = False\n",
    "    model_string = model_string + '_multiclass'\n",
    "    \n",
    "    model_string = model_string + '_'+datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "    \n",
    "    # 新增顺序注意力参数\n",
    "    config['use_sequential_attention'] = use_sequential_attention\n",
    "    \n",
    "    return config, model_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize Prior samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: <function <lambda>.<locals>.<lambda> at 0x7f3ab517b8b0>\n"
     ]
    }
   ],
   "source": [
    "config, model_string = reload_config(longer=1, use_sequential_attention=True)\n",
    "\n",
    "config['bptt_extra_samples'] = None\n",
    "\n",
    "# diff\n",
    "config['output_multiclass_ordered_p'] = 0.\n",
    "del config['differentiable_hyperparameters']['output_multiclass_ordered_p']\n",
    "\n",
    "config['multiclass_type'] = 'rank'\n",
    "del config['differentiable_hyperparameters']['multiclass_type']\n",
    "\n",
    "config['sampling'] = 'normal' # vielleicht schlecht?\n",
    "del config['differentiable_hyperparameters']['sampling']\n",
    "\n",
    "config['pre_sample_causes'] = True\n",
    "# end diff\n",
    "\n",
    "config['multiclass_loss_type'] = 'nono' # 'compatible'\n",
    "config['normalize_to_ranking'] = False # False\n",
    "\n",
    "config['categorical_feature_p'] = .2 # diff: .0\n",
    "\n",
    "# turn this back on in a random search!?\n",
    "config['nan_prob_no_reason'] = .0\n",
    "config['nan_prob_unknown_reason'] = .0 # diff: .0\n",
    "config['set_value_to_nan'] = .1 # diff: 1.\n",
    "\n",
    "config['normalize_with_sqrt'] = False\n",
    "\n",
    "config['new_mlp_per_example'] = True\n",
    "config['prior_mlp_scale_weights_sqrt'] = True\n",
    "config['batch_size_per_gp_sample'] = None\n",
    "\n",
    "config['normalize_ignore_label_too'] = False\n",
    "\n",
    "config['differentiable_hps_as_style'] = False\n",
    "config['max_eval_pos'] = 1000\n",
    "\n",
    "config['random_feature_rotation'] = True\n",
    "config['rotate_normalized_labels'] = True\n",
    "\n",
    "config[\"mix_activations\"] = False # False heisst eig True\n",
    "\n",
    "config['emsize'] = 512\n",
    "config['nhead'] = config['emsize'] // 128\n",
    "config['bptt'] = 1024+128\n",
    "config['canonical_y_encoder'] = False\n",
    "\n",
    "    \n",
    "config['aggregate_k_gradients'] = 1\n",
    "config['batch_size'] = 1*config['aggregate_k_gradients']\n",
    "config['num_steps'] = 1024//config['aggregate_k_gradients']\n",
    "config['epochs'] = 130\n",
    "config['total_available_time_in_s'] = None #60*60*22 # 22 hours for some safety...\n",
    "\n",
    "config['train_mixed_precision'] = True\n",
    "config['efficient_eval_masking'] = True\n",
    "\n",
    "config_sample = evaluate_hypers(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = get_model(config_sample, device, should_train=False, verbose=2) # , state_dict=model[2].state_dict()\n",
    "#(hp_embedding, data, _), targets, single_eval_pos = next(iter(model[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using style prior: True\n",
      "Using cuda device\n",
      "epoch:130\n",
      "steps_per_epoch:1024\n",
      "batch_size:1\n",
      "aggregate_k_gradients:1\n",
      "梯度下降的batch_size:1\n",
      "num_datasets:133120\n",
      "train_mixed_precision:True\n",
      "TransformerModel(\n",
      "  (transformer_encoder): TransformerEncoderDiffInit(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (encoder): ModelWithAttention(\n",
      "    (tabnet_encoder): TabNetModel(\n",
      "      (sparsemax): Sparsemax()\n",
      "      (feature_transform_linear1): Linear(in_features=512, out_features=256, bias=False)\n",
      "      (BN): BatchNorm1d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
      "      (BN1): BatchNorm1d(256, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
      "      (feature_transform_linear2): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (feature_transform_linear3): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (feature_transform_linear4): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (mask_linear_layer): Linear(in_features=192, out_features=512, bias=False)\n",
      "      (BN2): BatchNorm1d(512, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
      "      (final_classifier_layer): Linear(in_features=64, out_features=2, bias=False)\n",
      "    )\n",
      "    (embedding_layer): Linear(in_features=64, out_features=512, bias=True)\n",
      "  )\n",
      "  (y_encoder): Linear(in_features=1, out_features=512, bias=True)\n",
      "  (pos_encoder): NoPositionalEncoding()\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (projection_layer): Linear(in_features=100, out_features=512, bias=True)\n",
      ")\n",
      "Using a Transformer with 26.28 M parameters\n",
      "数据属性详细信息:\n",
      "data 元组的长度: 3\n",
      "\n",
      "data[0] 属性:\n",
      "类型: <class 'NoneType'>\n",
      "\n",
      "data[1] 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1, 100])\n",
      "数据类型: torch.float32\n",
      "设备: cuda:0\n",
      "最小值: -28.50568962097168\n",
      "最大值: 28.51473045349121\n",
      "\n",
      "data[2] 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1])\n",
      "数据类型: torch.float32\n",
      "设备: cuda:0\n",
      "最小值: 0.0\n",
      "最大值: 1.0\n",
      "\n",
      "targets 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1])\n",
      "数据类型: torch.float32\n",
      "\n",
      "single_eval_pos 属性:\n",
      "类型: <class 'int'>\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 83.63s | mean loss  2.28 | pos losses  2.26, 2.31,  nan,  nan,  nan,  nan,  nan,  nan, 2.26, 2.29, 2.29, 2.36,  nan,  nan,  nan,  nan, 2.33,  nan, 2.33, 2.29, 2.32, 2.28, 2.30,  nan, 2.31,  nan,  nan, 2.34,  nan, 2.28, 2.30, 2.35, 2.39, 2.41, 2.31,  nan,  nan, 2.32, 2.28,  nan, 2.32,  nan,  nan, 0.00,  nan, 2.33, 2.27, 2.30, 2.30, 2.33, 2.30, 2.30, 2.31, 2.30,  nan,  nan, 2.27,  nan, 2.31, 2.31, 2.33, 2.32, 2.34, 2.23, 2.29, 2.32,  nan, 2.31, 2.27, 2.32,  nan, 2.34,  nan, 2.33, 2.32,  nan, 2.33,  nan, 2.28,  nan, 2.29, 2.27,  nan,  nan, 2.30,  nan, 2.32, 2.29,  nan,  nan,  nan,  nan, 2.33, 2.33, 2.26,  nan, 2.35, 0.00, 2.34, 2.29,  nan, 2.23,  nan,  nan,  nan, 2.26,  nan, 2.30, 2.34, 2.33,  nan, 2.29,  nan, 2.28, 2.30, 2.31, 2.33, 2.29,  nan, 2.36,  nan,  nan, 2.32,  nan, 2.31,  nan,  nan, 2.34, 2.29, 2.30, 2.31,  nan, 2.32,  nan,  nan, 2.33, 2.31, 2.29,  nan, 2.34,  nan, 2.31,  nan,  nan, 2.38, 2.29, 2.25, 2.31, 2.33, 2.30, 2.33,  nan,  nan, 2.38,  nan,  nan, 2.35, 2.33, 2.30, 1.16, 2.33, 2.36,  nan,  nan, 2.33, 2.32, 2.29, 2.24, 2.27,  nan, 2.31, 2.34, 2.31, 2.40, 2.31,  nan, 2.33,  nan,  nan,  nan, 2.20, 2.33, 2.32,  nan, 2.28, 2.33,  nan, 1.52, 2.31, 2.33,  nan,  nan,  nan,  nan, 1.53,  nan, 2.27,  nan, 2.25, 2.34, 2.23, 2.27,  nan, 2.30, 2.36,  nan, 2.31, 2.32, 2.31, 2.30, 2.34,  nan,  nan,  nan, 2.30, 2.31,  nan,  nan, 2.31,  nan, 2.30, 2.29, 2.35, 2.30, 2.32, 2.30,  nan,  nan, 2.34,  nan, 2.28, 2.32,  nan,  nan, 2.30, 2.33,  nan, 2.31, 2.30,  nan, 2.30, 2.27,  nan,  nan, 2.30,  nan, 2.31, 2.21,  nan, 2.34, 2.26,  nan,  nan,  nan,  nan, 2.31,  nan, 2.33, 2.34, 2.33, 2.30, 2.31, 2.32,  nan, 2.31,  nan, 2.30,  nan,  nan, 2.33, 1.14, 2.35,  nan, 2.26, 2.32, 2.36, 2.29, 2.28,  nan,  nan, 2.33, 2.35,  nan,  nan, 2.39, 1.52, 2.35, 2.32, 2.32, 2.18, 2.29, 2.30,  nan, 2.29,  nan, 2.31, 2.33, 2.32,  nan, 2.32,  nan, 2.36, 2.34, 2.28, 2.36, 2.30, 2.32, 2.22,  nan,  nan, 2.31, 2.30,  nan,  nan, 2.33,  nan, 2.35, 2.36, 2.30, 2.31, 2.33, 2.27, 2.23,  nan,  nan, 2.30, 2.22, 2.49,  nan, 2.28, 2.33, 2.31, 2.31,  nan, 2.28,  nan,  nan,  nan, 2.31, 2.30,  nan,  nan, 2.32, 2.32, 2.35,  nan,  nan, 2.30, 2.30,  nan,  nan, 2.30, 2.30, 2.31,  nan, 2.33, 2.25, 2.27, 2.30, 2.31, 2.34,  nan,  nan, 2.30, 2.27, 2.33,  nan, 2.31, 2.34,  nan, 2.32, 2.28,  nan,  nan, 2.31,  nan, 2.31, 2.36, 2.31,  nan,  nan, 2.32, 2.32, 2.30, 2.29,  nan, 2.30, 2.29, 2.33,  nan, 2.31, 2.30, 2.32, 2.38, 2.29, 2.29, 2.28,  nan,  nan, 2.27, 2.25, 2.33, 2.31,  nan, 2.32,  nan, 0.00,  nan,  nan,  nan, 2.41,  nan, 2.30, 2.29,  nan, 2.30,  nan,  nan, 2.33, 2.29, 2.30,  nan, 2.31, 2.30, 2.35, 2.34, 2.32, 2.25, 2.27, 2.26,  nan, 2.35, 2.32, 2.34, 2.36,  nan, 2.33, 2.27,  nan,  nan, 2.30, 2.31,  nan, 2.35, 2.28, 2.31, 2.32, 2.31, 2.34, 2.31, 2.33,  nan, 2.32, 2.31, 2.30,  nan, 2.26, 2.42, 2.30, 2.31,  nan, 2.36, 2.28,  nan, 2.32, 2.33, 2.33,  nan, 2.34,  nan,  nan, 1.55,  nan,  nan,  nan, 2.33, 2.29,  nan,  nan,  nan, 2.30,  nan,  nan, 2.30,  nan, 2.26, 2.28,  nan,  nan, 2.33, 2.30, 2.35, 2.32,  nan,  nan, 2.27, 2.30, 2.30,  nan,  nan,  nan,  nan, 2.25, 2.29,  nan,  nan, 2.30, 2.32,  nan, 2.31,  nan, 2.34,  nan, 2.26, 2.26,  nan, 2.32, 2.30, 2.33, 2.32, 2.31,  nan, 2.25,  nan, 2.37, 2.32, 2.35, 2.26,  nan, 2.28,  nan, 2.19, 2.29,  nan, 2.31,  nan, 2.35, 2.29,  nan,  nan, 2.31, 2.21, 2.30, 2.27, 2.27, 2.31, 2.29,  nan, 2.26, 2.33, 2.24, 2.33, 2.35, 2.29, 2.29, 2.29, 2.29, 2.25, 2.34, 2.31, 2.32,  nan, 2.34,  nan, 2.30,  nan, 2.36, 2.28, 2.31,  nan, 2.33,  nan, 2.29, 1.53, 2.29, 2.32, 2.29,  nan,  nan,  nan, 2.33,  nan, 2.33, 2.34, 2.29,  nan, 2.31, 2.35, 2.22, 2.33,  nan, 2.32, 2.28, 2.39, 2.30,  nan,  nan, 2.30,  nan,  nan,  nan, 2.33,  nan, 2.31, 2.30,  nan, 2.09,  nan, 2.24,  nan, 2.31,  nan,  nan,  nan,  nan,  nan, 2.30, 2.29,  nan, 2.32, 2.32, 2.30,  nan, 2.31, 2.32, 2.30, 2.31,  nan, 2.29, 2.34,  nan, 2.28, 2.36,  nan, 2.31, 2.28,  nan, 2.32, 2.33, 2.32, 2.31, 2.29,  nan,  nan,  nan, 2.34, 2.31,  nan,  nan, 2.32, 2.29,  nan, 2.28,  nan, 2.30, 2.28, 2.30, 2.26,  nan, 2.30,  nan,  nan, 2.35, 2.30, 2.29, 2.28,  nan,  nan, 2.31,  nan,  nan, 2.30, 2.31,  nan, 2.31, 1.54,  nan, 2.28, 2.36, 2.32, 2.32,  nan, 2.31, 2.30, 2.36,  nan, 2.26,  nan, 2.20, 2.28, 2.34, 2.32, 2.34, 2.33, 2.30,  nan,  nan,  nan, 2.32, 2.28, 2.29,  nan, 2.29, 2.31,  nan, 2.37,  nan,  nan, 2.30, 2.30, 2.31, 2.28, 2.30,  nan,  nan, 2.31,  nan, 2.32, 2.25, 2.23,  nan,  nan,  nan, 2.33, 2.32, 2.27, 2.34, 2.32, 2.34, 2.31, 2.31, 1.16,  nan,  nan, 2.30, 2.29,  nan, 2.31, 2.28,  nan, 2.34, 2.33,  nan, 2.27, 2.32, 2.31,  nan, 2.30, 2.30, 2.34, 2.34, 2.32, 2.34, 2.30, 2.30,  nan, 2.29, 2.32, 2.31,  nan, 2.29, 2.30,  nan, 2.31,  nan, 2.32,  nan, 2.27, 2.35,  nan, 2.33,  nan,  nan,  nan,  nan, 2.33,  nan, 2.31,  nan,  nan, 2.30, 2.30,  nan, 2.30,  nan,  nan, 2.32,  nan, 2.30,  nan,  nan,  nan,  nan, 2.30,  nan, 2.33, 2.32, 2.31,  nan,  nan,  nan, 2.32,  nan,  nan,  nan, 2.33,  nan, 2.31, 2.33, 2.24, 2.33,  nan,  nan, 2.30, 2.33, 2.34, 2.32, 2.31,  nan,  nan, 2.32, 2.31, 2.30, 2.35, 2.26, 2.33, 2.35, 2.32, 2.31, 2.34, 2.31, 2.25, 2.32, 2.28, 2.33,  nan,  nan,  nan, 2.30,  nan, 2.28, 2.34,  nan, 2.32, 2.31, 2.33, 2.31, 2.28, 2.27, 2.31, 2.34, 2.31,  nan,  nan, 2.30, 2.29,  nan,  nan, 2.28, 2.31, 2.31,  nan, 2.34, 2.32, 2.27, 2.33, 2.27, 2.30, 2.33, 2.29, 2.30, 2.31, 2.32, 2.39, 2.30,  nan,  nan, 2.30, 2.29, 2.36, 2.28, 2.30, 2.27, 2.30,  nan, 2.34,  nan,  nan,  nan,  nan, 2.30, 2.30, 2.33, 2.32, 2.27, 2.35, 2.30, 2.28,  nan,  nan, 2.32, 2.32,  nan,  nan, 2.32, 2.31, 2.30,  nan,  nan, 2.24, 2.31, 2.31, 2.30, 2.30, 2.31, 2.30, 2.30, 2.32, 2.24, 2.31, 2.32, 2.30, 2.32,  nan, 2.32,  nan, 2.27,  nan,  nan, 2.31, 2.35, 2.39, 2.27, 1.53, 2.31,  nan, 2.29,  nan,  nan,  nan,  nan,  nan, 2.28,  nan,  nan, 2.32, 2.32,  nan,  nan, 2.31,  nan,  nan, 2.29,  nan,  nan, 2.29, 2.33, 2.34,  nan, 2.28,  nan, 2.30, 2.29,  nan, 2.31, 2.27, 2.32, 2.32, 2.31,  nan,  nan, 2.31,  nan, 2.28, 2.23,  nan,  nan, 2.36,  nan,  nan, 2.25, 2.33,  nan,  nan, 2.33, 2.26, 2.24, 2.28, 2.30,  nan, 2.33, 2.30, 2.31,  nan, 2.28, 2.28, 2.26, 2.29,  nan, 2.31,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, lr 0.0 data time  0.02 step time  0.07 forward time  0.02 nan share  0.00 ignore share (for classification tasks) 0.0127\n",
      "-----------------------------------------------------------------------------------------\n",
      "数据属性详细信息:\n",
      "data 元组的长度: 3\n",
      "\n",
      "data[0] 属性:\n",
      "类型: <class 'NoneType'>\n",
      "\n",
      "data[1] 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1, 100])\n",
      "数据类型: torch.float32\n",
      "设备: cuda:0\n",
      "最小值: -7.049849987030029\n",
      "最大值: 7.325778007507324\n",
      "\n",
      "data[2] 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1])\n",
      "数据类型: torch.float32\n",
      "设备: cuda:0\n",
      "最小值: 0.0\n",
      "最大值: 1.0\n",
      "\n",
      "targets 属性:\n",
      "类型: <class 'torch.Tensor'>\n",
      "形状: torch.Size([1152, 1])\n",
      "数据类型: torch.float32\n",
      "\n",
      "single_eval_pos 属性:\n",
      "类型: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "train_function(config_sample, 1, add_name='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
